Container rutime interface change from docker to containerd.

First we will change it on all worker nodes then all master nodes

1.First cordon the node

kubectl cordon (node name)

2. drain the  node

kubectl drain (node name) --ignore-daemonsets

3. check which docker with docker ps 

4. check which ctr

   ctr namespace list

5. ctr --namespace moby container list (both containers are same)

6. systemctl stop kubelet

7. systemctl stop docker

8 Take the backup of /etc/containerd/config.toml 

mv /etc/containerd/config.toml /etc/containerd/config.toml_bkp

9.containerd config default | sudo tee /etc/containerd/config.toml

Modify the below lines where registery is there vi /etc/containerd/config.toml

    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."172.23.21.36:8080"]
          endpoint = ["http://172.23.21.36:8080"]
        [plugin."io.containerd.grpc.v1.cri".registry.mirrors."172.23.21.36:8080"]

9. systemctl restart containerd

10.systemctl status containerd

11. vi /var/lib/kubelet/kubeadm-flags.env

it will be like this

KUBELET_KUBEADM_ARGS="--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.6"

Edit this as

KUBELET_KUBEADM_ARGS="--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.6 --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock"

12. systemctl start kubelet

13. systemctl start docker

14.kubectl edit no <node-name>

Change the value of kubeadm.alpha.kubernetes.io/cri-socket from /var/run/dockershim.sock to the CRI socket path unix:///run/containerd/containerd.sock

15.kubectl uncordon (node name)

Now we will see it as cri containerd 

Revert Back steps:

1.cordon the node
2.drain the node
3.stop the kubelet ,containerd and docker
4.change the kubeadm-flags.env as default
5.move the backup file to config.toml
6.restart the containerd and docker
7. check the status of nodes

kubectl delete pods --all --all-namespaces

kubectl scale --replicas=1 deployment.apps/deploy-selfcare-popup -n selfcare

kubectl scale --replicas=2 deployment --namespace=selfcare --all

kubectl drain master-node --ignore-daemonsets --delete-emptydir-data --force

kubectl get pod --all-namespaces | grep Evicted | awk '{print $1}' | xargs kubectl delete pod --all-namespaces

kubectl delete pods --field-selector status.phase=Failed -n selfcare

for p in $(kubectl get pods -n selfcare| grep Terminating | awk '{print $1}'); do kubectl delete pod $p --grace-period=0 --force -n selfcare;done

kubectl get pod -n selfcare | awk '{print $1}'| xargs kubectl delete pod -n selfcare

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

pkgs.k8s.io

kubeadm config images list --image-repository pkgs.k8s.io/kubernetes

kubeadm init --image-repository=pkgs.k8s.io/kubernetes


curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list


